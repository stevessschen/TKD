"""
é‡‘èç•°å¸¸æª¢æ¸¬ - ä¸»æµç¨‹

æœ¬æ¨¡çµ„å¯¦ç¾äº†ä½¿ç”¨åœ–å·ç©ç¶²è·¯å’Œè®Šåˆ†è‡ªç·¨ç¢¼å™¨ (GCN-VAE) æª¢æ¸¬å¯ç–‘é‡‘èå¸³æˆ¶çš„ä¸»è¨“ç·´æµç¨‹ã€‚è©²è§£æ±ºæ–¹æ¡ˆé€éå¼•å…¥æ™‚é–“è¦–çª—ç‰¹å¾µä¾†æ•æ‰å¸³æˆ¶æ¨™è¨˜å‰çš„è¡Œç‚ºè®ŠåŒ–ï¼Œåœ¨ç§æœ‰æ’è¡Œæ¦œä¸Šå–å¾—äº† 0.34 çš„ F1 åˆ†æ•¸ã€‚

æ¶æ§‹æ¦‚è¿°ï¼š
- ç‰¹å¾µå·¥ç¨‹ï¼š27 ç¶­ç‰¹å¾µç©ºé–“
* åŸºæœ¬ç‰¹å¾µï¼ˆ10 ç¶­ï¼‰ï¼šåº¦ã€é‡‘é¡ã€è‡ªäº¤æ˜“
* æ™‚é–“ç‰¹å¾µï¼ˆ5 ç¶­ï¼‰ï¼šäº¤æ˜“æ™‚é–“æ¨¡å¼
* æ¨¡å¼ç‰¹å¾µï¼ˆ5 ç¶­ï¼‰ï¼šåˆä½œå¤¥ä¼´å¤šæ¨£æ€§ã€é‡‘é¡çµ±è¨ˆ
* æ™‚é–“çª—å£ç‰¹å¾µï¼ˆ6 ç¶­ï¼‰ï¼šæ—©æœŸèˆ‡æ™šæœŸè¡Œç‚ºè®ŠåŒ–
* PageRankï¼ˆ1 ç¶­ï¼‰ï¼šç¶²è·¯ä¸­å¿ƒæ€§
- æ¨¡å‹ï¼šæ¡ç”¨ 4 å±¤ç·¨ç¢¼å™¨çš„ GCN-VAE
- è¨“ç·´ï¼šä½¿ç”¨ Focal Loss é€²è¡Œå›°é›£è² æ¨£æœ¬æŒ–æ˜

ä¸»è¦å‰µæ–°ï¼š
1. æ™‚é–“çª—å£åˆ†æï¼šæ¯”è¼ƒæ—©æœŸï¼ˆç¬¬ 1-60 å¤©ï¼‰å’Œæ™šæœŸï¼ˆç¬¬ 91-121 å¤©ï¼‰
è¡Œç‚ºï¼Œä»¥æª¢æ¸¬é å…ˆæ¨™è¨˜çš„ç•°å¸¸
2. å›°é›£è² æ¨£æœ¬æŒ–æ˜ï¼šé¸æ“‡èˆ‡æ­£æ¨£æœ¬ç›¸ä¼¼çš„å…·æœ‰æŒ‘æˆ°æ€§çš„è² æ¨£æœ¬ï¼Œä»¥æé«˜æ¨¡å‹ç©©å¥æ€§
3. å¤šç›®æ¨™æå¤±ï¼šå¹³è¡¡é‡æ§‹ã€KL æ•£åº¦å’Œ
åˆ†é¡ï¼Œä¸¦ä½¿ç”¨ Focal Loss è§£æ±ºé¡åˆ¥ä¸å¹³è¡¡å•é¡Œ
"""
# ============================================================================
# ç¬¬1æ¬¡æ–¹æ¡ˆï¼šæ™‚åºçª—å£å¢å¼·ç‰ˆ
# åŸºæ–¼ï¼šF1-0.277.py
# æ–°å¢ï¼š6ç¶­æ™‚åºçª—å£ç‰¹å¾µ
# ç›®æ¨™ï¼šF1 0.30-0.35
# 
# æ ¸å¿ƒæ”¹é€²ï¼š
# 1. æ™‚åºçª—å£åˆ†æï¼ˆæ—©æœŸ vs æ™šæœŸè¡Œç‚ºï¼‰
# 2. è®ŠåŒ–ç‡ç‰¹å¾µï¼ˆæŠ“ä½ã€Œå³å°‡è¢«æ¨™è¨˜å‰ã€çš„ç•°å¸¸è®ŠåŒ–ï¼‰
# 3. æœ€å¾Œäº¤æ˜“æ—¥ç‰¹å¾µï¼ˆè¶Šæ¥è¿‘day 121è¶Šå¯ç–‘ï¼‰
# ============================================================================

from numba import njit, prange
import time

import os
import numpy as np
import pandas as pd
import scipy.sparse as sp
from datetime import datetime
from tqdm import tqdm, trange
import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix

print("âœ“ å¥—ä»¶è¼‰å…¥å®Œæˆ")
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

# ============================================================================
# Cell 2: åƒæ•¸è¨­å®šï¼ˆä¿æŒF1-0.277çš„æˆåŠŸåƒæ•¸ï¼‰
# ============================================================================

TRAIN_CSV = './dataset/acct_transaction.csv'
ALERT_CSV = './dataset/acct_alert.csv'
PREDICT_CSV = './dataset/acct_predict.csv'
OUTPUT_CSV = './output/predictions_plan1_temporal.csv'
CHECKPOINT_DIR = './checkpoints_plan1'

# === æ¨¡å‹åƒæ•¸ï¼ˆF1-0.277çš„æˆåŠŸé…ç½®ï¼‰===
HIDDEN_DIMS = [128, 64, 32, 16]  # GCN layer dimensions
DROPOUT = 0.5  # Dropout rate for regularization
EPOCHS = 150  # Maximum number of training epochs
LEARNING_RATE = 0.01  # Adam optimizer learning rate
WEIGHT_DECAY = 5e-4  # L2 regularization weight
PATIENCE = 30  # Early stopping patience (critical parameter for F1 0.34)

# === æå¤±æ¬Šé‡ ===
KL_WEIGHT = 0.3  # Weight for KL divergence loss in VAE
CLS_WEIGHT = 5.0  # Weight for classification loss (handles class imbalance)
FOCAL_ALPHA = 0.75  # Focal loss alpha (focus on hard examples)
FOCAL_GAMMA = 2.0  # Focal loss gamma (down-weight easy examples)

# === è³‡æ–™åƒæ•¸ ===
NEG_RATIO = 5.0  # Negative to positive sample ratio
EDGE_SAMPLES = 8000  # Number of graph edges to sample per training epoch
MAX_EDGES = None  # Maximum edges to load (None = load all)

# === å…¶ä»– ===
USE_ENSEMBLE = False  # Whether to use model ensemble (not implemented)
RANDOM_SEED = 42  # Random seed for reproducibility
DEVICE = 'cpu'  # Computing device ('cpu')

# Create output directories
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)

print("="*70)
print("  ğŸ¯ ç¬¬1æ¬¡æ–¹æ¡ˆï¼šæ™‚åºçª—å£å¢å¼·ç‰ˆ")
print("="*70)
print(f"åŸºæ–¼ï¼šF1-0.277.py æˆåŠŸé…ç½®")
print(f"æ–°å¢ï¼š6ç¶­æ™‚åºçª—å£ç‰¹å¾µ")
print(f"ç¸½ç‰¹å¾µï¼š21 + 6 = 27ç¶­")
print(f"")
print(f"æ™‚åºçª—å£è¨­è¨ˆï¼š")
print(f"  - æ—©æœŸï¼šday 1-60")
print(f"  - ä¸­æœŸï¼šday 61-90")
print(f"  - æ™šæœŸï¼šday 91-121 â­ é—œéµçª—å£")
print(f"")
print(f"ç›®æ¨™ï¼šF1 0.30-0.35")
print("="*70)

# ============================================================================
# åŸ·è¡Œä¸»ç¨‹åº
# ============================================================================

from Preprocess.data_preprocess import (
    load_transaction_data,
    build_account_mapping,
    build_all_features
)

from Model.model import (
    build_labels_with_hard_negatives,
    sparse_to_torch,
    train_model,
    predict
)

print("\n" + "="*70)
print("  éšæ®µ 1: æ•¸æ“šè¼‰å…¥")
print("="*70)

df_txn = load_transaction_data(TRAIN_CSV, max_edges=MAX_EDGES)
id2idx, idx2id = build_account_mapping(df_txn)

print("\n" + "="*70)
print("  éšæ®µ 2: ç‰¹å¾µå·¥ç¨‹")
print("="*70)

features, adj, adj_norm = build_all_features(df_txn, id2idx)

print(f"\næœ€çµ‚æ•¸æ“šæ‘˜è¦:")
print(f"  - ç¯€é»æ•¸: {len(id2idx):,}")
print(f"  - é‚Šæ•¸: {adj.nnz:,}")
print(f"  - ç‰¹å¾µç¶­åº¦: {features.shape}")

print("\n" + "="*70)
print("  éšæ®µ 3: å»ºç«‹æ¨™ç±¤")
print("="*70)

train_idx, train_y, val_idx, val_y = build_labels_with_hard_negatives(
    ALERT_CSV, id2idx, features, neg_ratio=NEG_RATIO, seed=RANDOM_SEED, val_ratio=0.2
)

print("\n" + "="*70)
print("  éšæ®µ 4: æ¨¡å‹è¨“ç·´")
print("="*70)

config = {
    'device': DEVICE, 'hidden_dims': HIDDEN_DIMS, 'dropout': DROPOUT,
    'epochs': EPOCHS, 'lr': LEARNING_RATE, 'weight_decay': WEIGHT_DECAY,
    'patience': PATIENCE, 'kl_weight': KL_WEIGHT, 'cls_weight': CLS_WEIGHT,
    'focal_alpha': FOCAL_ALPHA, 'focal_gamma': FOCAL_GAMMA, 'edge_samples': EDGE_SAMPLES
}

model, clf, best_threshold, best_f1 = train_model(
    features, adj, adj_norm, train_idx, train_y, val_idx, val_y, config, seed=RANDOM_SEED
)

print("\nâœ“ è¨“ç·´å®Œæˆï¼")

print("\n" + "="*70)
print("  éšæ®µ 5: é©—è­‰é›†è©•ä¼°")
print("="*70)

device = torch.device(DEVICE)
features_t = torch.tensor(features, dtype=torch.float32, device=device)
adj_norm_t = sparse_to_torch(adj_norm).coalesce().to(device)
val_idx_t = torch.tensor(val_idx, dtype=torch.long, device=device)
val_y_t = torch.tensor(val_y, dtype=torch.float32, device=device)

predictions, probs = predict(model, clf, features_t, adj_norm_t, val_idx_t, best_threshold)

val_y_np = val_y_t.cpu().numpy()
p, r, f1, _ = precision_recall_fscore_support(val_y_np, predictions, average='binary', zero_division=0)

print(f"\næœ€çµ‚é©—è­‰é›†çµæœ (é–¾å€¼={best_threshold:.2f}):")
print(f"  Precision: {p:.4f}")
print(f"  Recall: {r:.4f}")
print(f"  F1 Score: {f1:.4f}")

print("\n" + "="*70)
print("  éšæ®µ 6: é æ¸¬")
print("="*70)

df_pred = pd.read_csv(PREDICT_CSV)
accts = df_pred['acct'].astype(str).tolist()
print(f"  âœ“ è¼‰å…¥ {len(accts):,} å€‹å¾…é æ¸¬å¸³æˆ¶")

pred_indices = []
for acct in accts:
    if acct in id2idx:
        pred_indices.append(id2idx[acct])
    else:
        pred_indices.append(-1)

valid_mask = np.array(pred_indices) >= 0
valid_indices = np.array([i for i in pred_indices if i >= 0])

valid_indices_t = torch.tensor(valid_indices, dtype=torch.long, device=device)
predictions_valid, probs_valid = predict(model, clf, features_t, adj_norm_t, valid_indices_t, best_threshold)

predictions = np.zeros(len(accts), dtype=int)
probs_all = np.zeros(len(accts), dtype=float)

valid_idx = 0
for i, is_valid in enumerate(valid_mask):
    if is_valid:
        predictions[i] = predictions_valid[valid_idx]
        probs_all[i] = probs_valid[valid_idx]
        valid_idx += 1

result_df = pd.DataFrame({'acct': accts, 'label': predictions})
result_df.to_csv(OUTPUT_CSV, index=False)

print(f"\nâœ“ é æ¸¬çµæœå·²å„²å­˜è‡³: {OUTPUT_CSV}")
print(f"\né æ¸¬çµ±è¨ˆ:")
print(f"  é æ¸¬ç‚ºç•°å¸¸: {predictions.sum():,} ({predictions.sum()/len(accts)*100:.2f}%)")

print("\n" + "="*70)
print("  ğŸ¯ ç¬¬1æ¬¡æ–¹æ¡ˆåŸ·è¡Œå®Œæˆï¼")
print("="*70)
print(f"é©—è­‰é›†F1: {f1:.4f}")
print(f"æœ€ä½³é–¾å€¼: {best_threshold:.2f}")
print(f"è¼¸å‡ºæª”æ¡ˆ: {OUTPUT_CSV}")
print(f"")
print(f"â­ æ–°å¢æ™‚åºç‰¹å¾µï¼š")
print(f"  1. æ—©æœŸäº¤æ˜“é »ç‡")
print(f"  2. æ™šæœŸäº¤æ˜“é »ç‡")
print(f"  3. é »ç‡è®ŠåŒ–ç‡ (æŠ“ä½çªç„¶æ´»èº)")
print(f"  4. æ™šæœŸå¹³å‡é‡‘é¡")
print(f"  5. é‡‘é¡è®ŠåŒ–ç‡ (æŠ“ä½çªç„¶å¤§é¡)")
print(f"  6. æœ€å¾Œäº¤æ˜“æ—¥ (è¶Šæ¥è¿‘day 121è¶Šå¯ç–‘)")

print("="*70)
